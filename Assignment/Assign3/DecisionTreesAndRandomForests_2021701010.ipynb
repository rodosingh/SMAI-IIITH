{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0_wToNFHMN3"
   },
   "source": [
    "# **Decision Trees**\n",
    "\n",
    "The Wisconsin Breast Cancer Dataset(WBCD) can be found here(https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data)\n",
    "\n",
    "This dataset describes the characteristics of the cell nuclei of various patients with and without breast cancer. The task is to classify a decision tree to predict if a patient has a benign or a malignant tumour based on these features.\n",
    "\n",
    "Attribute Information:\n",
    "```\n",
    "#  Attribute                     Domain\n",
    "   -- -----------------------------------------\n",
    "   1. Sample code number            id number\n",
    "   2. Clump Thickness               1 - 10\n",
    "   3. Uniformity of Cell Size       1 - 10\n",
    "   4. Uniformity of Cell Shape      1 - 10\n",
    "   5. Marginal Adhesion             1 - 10\n",
    "   6. Single Epithelial Cell Size   1 - 10\n",
    "   7. Bare Nuclei                   1 - 10\n",
    "   8. Bland Chromatin               1 - 10\n",
    "   9. Normal Nucleoli               1 - 10\n",
    "  10. Mitoses                       1 - 10\n",
    "  11. Class:                        (2 for benign, 4 for malignant)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "JD3Cj7rBNB2G"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "qYdlWpUVHMOB",
    "outputId": "f1cb8f0b-01e4-41f7-b6b2-5f0aa6182fc1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CT</th>\n",
       "      <th>UCSize</th>\n",
       "      <th>UCShape</th>\n",
       "      <th>MA</th>\n",
       "      <th>SECSize</th>\n",
       "      <th>BN</th>\n",
       "      <th>BC</th>\n",
       "      <th>NN</th>\n",
       "      <th>Mitoses</th>\n",
       "      <th>Diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.417740</td>\n",
       "      <td>3.134478</td>\n",
       "      <td>3.207439</td>\n",
       "      <td>2.806867</td>\n",
       "      <td>3.216023</td>\n",
       "      <td>3.463519</td>\n",
       "      <td>3.437768</td>\n",
       "      <td>2.866953</td>\n",
       "      <td>1.589413</td>\n",
       "      <td>2.689557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.815741</td>\n",
       "      <td>3.051459</td>\n",
       "      <td>2.971913</td>\n",
       "      <td>2.855379</td>\n",
       "      <td>2.214300</td>\n",
       "      <td>3.640708</td>\n",
       "      <td>2.438364</td>\n",
       "      <td>3.053634</td>\n",
       "      <td>1.715078</td>\n",
       "      <td>0.951273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               CT      UCSize     UCShape          MA     SECSize          BN  \\\n",
       "count  699.000000  699.000000  699.000000  699.000000  699.000000  699.000000   \n",
       "mean     4.417740    3.134478    3.207439    2.806867    3.216023    3.463519   \n",
       "std      2.815741    3.051459    2.971913    2.855379    2.214300    3.640708   \n",
       "min      1.000000    1.000000    1.000000    1.000000    1.000000    0.000000   \n",
       "25%      2.000000    1.000000    1.000000    1.000000    2.000000    1.000000   \n",
       "50%      4.000000    1.000000    1.000000    1.000000    2.000000    1.000000   \n",
       "75%      6.000000    5.000000    5.000000    4.000000    4.000000    5.000000   \n",
       "max     10.000000   10.000000   10.000000   10.000000   10.000000   10.000000   \n",
       "\n",
       "               BC          NN     Mitoses   Diagnosis  \n",
       "count  699.000000  699.000000  699.000000  699.000000  \n",
       "mean     3.437768    2.866953    1.589413    2.689557  \n",
       "std      2.438364    3.053634    1.715078    0.951273  \n",
       "min      1.000000    1.000000    1.000000    2.000000  \n",
       "25%      2.000000    1.000000    1.000000    2.000000  \n",
       "50%      3.000000    1.000000    1.000000    2.000000  \n",
       "75%      5.000000    4.000000    1.000000    4.000000  \n",
       "max     10.000000   10.000000   10.000000    4.000000  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = [\"ID\", \"CT\", \"UCSize\", \"UCShape\", \"MA\",\\\n",
    "           \"SECSize\", \"BN\", \"BC\", \"NN\", \"Mitoses\", \"Diagnosis\"]\n",
    "data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data', \n",
    "                   na_values='?', \n",
    "                   header=None, \n",
    "                   index_col=['ID'], \n",
    "                   names = headers) \n",
    "data = data.reset_index(drop=True)\n",
    "data = data.fillna(0)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "no5JsmzcO5PF",
    "outputId": "07286be0-fc2c-4eae-cc10-f6b9b99c24ae"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CT</th>\n",
       "      <th>UCSize</th>\n",
       "      <th>UCShape</th>\n",
       "      <th>MA</th>\n",
       "      <th>SECSize</th>\n",
       "      <th>BN</th>\n",
       "      <th>BC</th>\n",
       "      <th>NN</th>\n",
       "      <th>Mitoses</th>\n",
       "      <th>Diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CT  UCSize  UCShape  MA  SECSize    BN  BC  NN  Mitoses  Diagnosis\n",
       "0   5       1        1   1        2   1.0   3   1        1          2\n",
       "1   5       4        4   5        7  10.0   3   2        1          2\n",
       "2   3       1        1   1        2   2.0   3   1        1          2\n",
       "3   6       8        8   1        3   4.0   3   7        1          2\n",
       "4   4       1        1   3        2   1.0   3   1        1          2\n",
       "5   8      10       10   8        7  10.0   9   7        1          4\n",
       "6   1       1        1   1        2  10.0   3   1        1          2\n",
       "7   2       1        2   1        2   1.0   3   1        1          2\n",
       "8   2       1        1   1        2   1.0   1   1        5          2\n",
       "9   4       2        1   1        2   1.0   2   1        1          2"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show first 10 columns of data\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1qSzou0kM8JR",
    "outputId": "b902c95e-f1a6-4eb6-aa9a-d3e2ea3198df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CT           0\n",
       "UCSize       0\n",
       "UCShape      0\n",
       "MA           0\n",
       "SECSize      0\n",
       "BN           0\n",
       "BC           0\n",
       "NN           0\n",
       "Mitoses      0\n",
       "Diagnosis    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking for empty cells.\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "zYyMkHOXPFSl"
   },
   "outputs": [],
   "source": [
    "# Define Features and labels i.e., X-data and Y-data\n",
    "label = data['Diagnosis'].values\n",
    "Features = data.drop('Diagnosis', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Features, label, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_gQq5qrHMOG"
   },
   "source": [
    "1. a) Implement a decision tree (you can use decision tree implementation from existing libraries).\n",
    "\n",
    "   b) Train a decision tree object of the above class on the WBC dataset using **misclassification rate**, **entropy** and **Gini** as the splitting metrics.\n",
    "\n",
    "   c) Report the accuracies in each of the above splitting metrics and give the best result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\LARGE 1a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've defined noly for Misclassification rate.\n",
    "\n",
    "def misclassification_rate(prob):\n",
    "    misc_rate = 1 - max(prob, 1-prob)\n",
    "\n",
    "    return misc_rate\n",
    "\n",
    "def misclassification_score(left_child_node, right_child_node):    \n",
    "    parent = left_child_node + right_child_node\n",
    "\n",
    "    prob_left = left_child_node.count(1) / len(left_child_node) if len(left_child_node) > 0 else 0\n",
    "    prob_right = right_child_node.count(1) / len(right_child_node) if len(right_child_node) > 0 else 0\n",
    "\n",
    "    misc_score_left = misclassification_rate(prob_left)\n",
    "    misc_score_right = misclassification_rate(prob_right)\n",
    "\n",
    "    return (len(left_child_node)/len(parent))*misc_score_left + (len(right_child_node)/len(parent))*misc_score_right\n",
    "\n",
    "def find_split_point(X_bootstrap, y_bootstrap, max_features):\n",
    "    \"\"\"Given maximum no. of features at a node try to find the best split point according\n",
    "    to impurity score for each threshold or possible split point.\"\"\"\n",
    "    feature_ls = list()\n",
    "    num_features = len(X_bootstrap[0])\n",
    "    \n",
    "    while len(feature_ls) <= max_features:\n",
    "        feature_idx = random.sample(range(num_features), 1)\n",
    "        if feature_idx not in feature_ls:\n",
    "            feature_ls.extend(feature_idx)\n",
    "    \n",
    "    best_info_gain = -999\n",
    "    node = None\n",
    "    for feature_idx in feature_ls:\n",
    "        for split_point in X_bootstrap[:,feature_idx]:\n",
    "            left_child_node = {'X_bootstrap': [], 'y_bootstrap': []}\n",
    "            right_child_node = {'X_bootstrap': [], 'y_bootstrap': []}\n",
    "            \n",
    "            # split children for continuous variables\n",
    "            if type(split_point) in [int, float]:\n",
    "                for i, value in enumerate(X_bootstrap[:,feature_idx]):\n",
    "                    if value <= split_point:\n",
    "                        left_child_node['X_bootstrap'].append(X_bootstrap[i])\n",
    "                        left_child_node['y_bootstrap'].append(y_bootstrap[i])\n",
    "                    else:\n",
    "                        right_child_node['X_bootstrap'].append(X_bootstrap[i])\n",
    "                        right_child_node['y_bootstrap'].append(y_bootstrap[i])\n",
    "            # split children for categoric variables\n",
    "            else:\n",
    "                for i, value in enumerate(X_bootstrap[:,feature_idx]):\n",
    "                    if value == split_point:\n",
    "                        left_child_node['X_bootstrap'].append(X_bootstrap[i])\n",
    "                        left_child_node['y_bootstrap'].append(y_bootstrap[i])\n",
    "                    else:\n",
    "                        right_child_node['X_bootstrap'].append(X_bootstrap[i])\n",
    "                        right_child_node['y_bootstrap'].append(y_bootstrap[i])\n",
    "            \n",
    "            split_info_gain = misclassification_score(left_child_node['y_bootstrap'],\\\n",
    "                                                      right_child_node['y_bootstrap'])\n",
    "            \n",
    "            if split_info_gain > best_info_gain:\n",
    "                best_info_gain = split_info_gain\n",
    "                left_child_node['X_bootstrap'] = np.array(left_child_node['X_bootstrap'])\n",
    "                right_child_node['X_bootstrap'] = np.array(right_child_node['X_bootstrap'])\n",
    "                node = {'information_gain': split_info_gain, \n",
    "                        'left_child': left_child_node, \n",
    "                        'right_child': right_child_node, \n",
    "                        'split_point': split_point,\n",
    "                        'feature_idx': feature_idx}\n",
    "                \n",
    "    \n",
    "    return node\n",
    "\n",
    "def terminal_node(node):\n",
    "    \"\"\"Based on max count of labels/class, assign the label to leaf node\"\"\"\n",
    "    y_bootstrap = node['y_bootstrap']\n",
    "    pred = max(y_bootstrap, key = y_bootstrap.count)\n",
    "    return pred\n",
    "\n",
    "\n",
    "def split_node(node, max_features, min_samples_split, max_depth, depth):\n",
    "    \"\"\"Based on Impurity score try to split the node \n",
    "    subject to constraints like max_features, min_samples_spit, max_depth and depth.\"\"\"\n",
    "    left_child = node['left_child']\n",
    "    right_child = node['right_child']\n",
    "\n",
    "    del(node['left_child'])\n",
    "    del(node['right_child'])\n",
    "\n",
    "    if len(left_child['y_bootstrap']) == 0 or len(right_child['y_bootstrap']) == 0:\n",
    "        empty_child = {'y_bootstrap': left_child['y_bootstrap'] + right_child['y_bootstrap']}\n",
    "        node['left_split'] = terminal_node(empty_child)\n",
    "        node['right_split'] = terminal_node(empty_child)\n",
    "        return\n",
    "\n",
    "    if depth >= max_depth:\n",
    "        node['left_split'] = terminal_node(left_child)\n",
    "        node['right_split'] = terminal_node(right_child)\n",
    "        return node\n",
    "\n",
    "    if len(left_child['X_bootstrap']) <= min_samples_split:\n",
    "        node['left_split'] = node['right_split'] = terminal_node(left_child)\n",
    "    else:\n",
    "        node['left_split'] = find_split_point(left_child['X_bootstrap'], left_child['y_bootstrap'], max_features)\n",
    "        split_node(node['left_split'], max_depth, min_samples_split, max_depth, depth + 1)\n",
    "    if len(right_child['X_bootstrap']) <= min_samples_split:\n",
    "        node['right_split'] = node['left_split'] = terminal_node(right_child)\n",
    "    else:\n",
    "        node['right_split'] = find_split_point(right_child['X_bootstrap'], right_child['y_bootstrap'], max_features)\n",
    "        split_node(node['right_split'], max_features, min_samples_split, max_depth, depth + 1)\n",
    "        \n",
    "def build_tree(X_bootstrap, y_bootstrap, max_depth, min_samples_split, max_features):\n",
    "    \"\"\"Build decision tree given boostrapped data along with its labels \n",
    "    and the criteria to split + some extra factors.\"\"\"\n",
    "    root_node = find_split_point(X_bootstrap, y_bootstrap, max_features)\n",
    "    split_node(root_node, max_features, min_samples_split, max_depth, 1)\n",
    "    return root_node        \n",
    "        \n",
    "def predict_tree(tree, x_test_sample):\n",
    "    \"\"\"Predict the output for a single sample with a given tree.\"\"\"\n",
    "    feature_idx = tree['feature_idx']\n",
    "\n",
    "    if x_test_sample[feature_idx] <= tree['split_point']:\n",
    "        if type(tree['left_split']) == dict:\n",
    "            return predict_tree(tree['left_split'], x_test_sample)\n",
    "        else:\n",
    "            value = tree['left_split']\n",
    "            return value\n",
    "    else:\n",
    "        if type(tree['right_split']) == dict:\n",
    "            return predict_tree(tree['right_split'], x_test_sample)\n",
    "        else:\n",
    "            return tree['right_split']\n",
    "        \n",
    "def predict_all_tree(tree, X_test):\n",
    "    \"\"\"Return prediction for all samples in X_test\"\"\"\n",
    "    if type(X_test) != np.ndarray:\n",
    "        X_test = X_test.values\n",
    "    return np.array([predict_tree(tree, X_test[i]) for i in range(len(X_test))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\LARGE 1b)\\ \\ \\&\\ \\ \\LARGE 1c)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "g6R3GmzBHMOH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Gini as splitting metrics, the accuracy score =  0.935064935064935\n"
     ]
    }
   ],
   "source": [
    "Decision_Tree = DecisionTreeClassifier(criterion=\"gini\")\n",
    "Decision_Tree.fit(X_train,y_train)\n",
    "pred = Decision_Tree.predict(X_test)\n",
    "print(\"With Gini as splitting metrics, the accuracy score = \", accuracy_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BLCFfD6kQJjt",
    "outputId": "9b8f72bf-5be3-4b88-c46e-183b99ed9a6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Entropy as splitting metrics, the accuracy score =  0.9393939393939394\n"
     ]
    }
   ],
   "source": [
    "Decision_Tree = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "Decision_Tree.fit(X_train,y_train)\n",
    "pred = Decision_Tree.predict(X_test)\n",
    "print(\"With Entropy as splitting metrics, the accuracy score = \", accuracy_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Misclassification rate as splitting metrics, the accuracy score =  0.8917748917748918\n"
     ]
    }
   ],
   "source": [
    "model_dt = build_tree(X_train.values, y_train, max_features=3, max_depth=3, min_samples_split=2)\n",
    "\n",
    "preds = predict_all_tree(model_dt, X_test)\n",
    "print(\"With Misclassification rate as splitting metrics, the accuracy score = \", accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bz_7nYxPHMON"
   },
   "source": [
    "$\\LARGE 1d)$\n",
    "\n",
    "1. d) Experiment with different approaches to decide when to terminate the tree (number of layers, purity measure, etc). Report and give explanations for all approaches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for entropy splitting metric with max_depth=7 and min limit for decrease in impurity=0.0015, is 0.935064935064935 \n",
      "\n",
      "Accuracy for gini splitting metric with max_depth=7 and min limit for decrease in impurity=0.0015, is 0.9523809523809523 \n",
      "\n",
      "Accuracy for entropy splitting metric with max_depth=9 and max_features=sqrt is 0.9307359307359307 \n",
      "\n",
      "Accuracy for gini splitting metric with max_depth=9 and max_features=sqrt is 0.935064935064935 \n",
      "\n",
      "Accuracy for entropy splitting metric with max_depth=9 and max_features=log2 is 0.9567099567099567 \n",
      "\n",
      "Accuracy for gini splitting metric with max_depth=9 and max_features=log2 is 0.9523809523809523 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "decision_tree_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=7, min_impurity_decrease=0.0015)\n",
    "decision_tree_entropy.fit(X_train, y_train)\n",
    "entropy_preds = decision_tree_entropy.predict(X_test)\n",
    "print('Accuracy for entropy splitting metric with max_depth=7 and min limit for decrease in impurity=0.0015, is', accuracy_score(y_test, entropy_preds), '\\n')\n",
    "\n",
    "decision_tree_gini = DecisionTreeClassifier(criterion='gini', max_depth=7, min_impurity_decrease=0.0015)\n",
    "decision_tree_gini.fit(X_train, y_train)\n",
    "gini_preds = decision_tree_gini.predict(X_test)\n",
    "print('Accuracy for gini splitting metric with max_depth=7 and min limit for decrease in impurity=0.0015, is', accuracy_score(y_test, gini_preds), '\\n')\n",
    "\n",
    "decision_tree_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=9, max_features='sqrt')\n",
    "decision_tree_entropy.fit(X_train, y_train)\n",
    "entropy_preds = decision_tree_entropy.predict(X_test)\n",
    "print('Accuracy for entropy splitting metric with max_depth=9 and max_features=sqrt is', accuracy_score(y_test, entropy_preds), '\\n')\n",
    "\n",
    "decision_tree_gini = DecisionTreeClassifier(criterion='gini', max_depth=9, max_features='sqrt')\n",
    "decision_tree_gini.fit(X_train, y_train)\n",
    "gini_preds = decision_tree_gini.predict(X_test)\n",
    "print('Accuracy for gini splitting metric with max_depth=9 and max_features=sqrt is', accuracy_score(y_test, gini_preds), '\\n')\n",
    "\n",
    "decision_tree_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=9, max_features='log2')\n",
    "decision_tree_entropy.fit(X_train, y_train)\n",
    "entropy_preds = decision_tree_entropy.predict(X_test)\n",
    "print('Accuracy for entropy splitting metric with max_depth=9 and max_features=log2 is', accuracy_score(y_test, entropy_preds), '\\n')\n",
    "\n",
    "decision_tree_gini = DecisionTreeClassifier(criterion='gini', max_depth=9, max_features='log2')\n",
    "decision_tree_gini.fit(X_train, y_train)\n",
    "gini_preds = decision_tree_gini.predict(X_test)\n",
    "print('Accuracy for gini splitting metric with max_depth=9 and max_features=log2 is', accuracy_score(y_test, gini_preds), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uWAN_wWXHMOQ"
   },
   "source": [
    "2. What is boosting, bagging and  stacking? Which class does random forests belong to and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnO5uqHlHMOR"
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "1. **Boosting**:- Boosting is an ensemble iterative technique where homogenous weak learners **learn sequentially in an adaptive manner**, i.e., it adjusts the weights of observations based on the last classification. In other words it aims at solving the problem by training **a set of weak learners in series** with each learner learning the errors from previous learner and try to correct them.\n",
    "\n",
    "2. **Bagging**:- A method homogenous weak learners learns independently from each other as a small sample population and combines their prediction following some deterministic averaging process. Informally, it is an ensemble technique which aims at solving the problem by training a set of weak learners in **parallel and combines the output** from all the learners to predict the final output.\n",
    "\n",
    "3. **Stacking**:- Stacking is an ensemble technique which aims at solving problem by training a set of different models (may be homogenous or heterogenous weak learners) at intermediate level in parallel, and then train a final model (also called **meta model**) which learns from these intermediate models' output to predict the final output.\n",
    "\n",
    "*Random Forest use bagging technique. They build a bunch of decision trees as weak learners then combine their outputs (through voting) to generate the final predictions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pihvGbqLHMOS"
   },
   "source": [
    "3. Implement random forest algorithm using different decision trees . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dXdPP2aIHMOT",
    "outputId": "648525a5-e1b1-4bdb-ae58-94ea5bcc1a3d"
   },
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    \"\"\"Shannon's entropy score.\"\"\"\n",
    "    if p == 0:\n",
    "        return 0\n",
    "    elif p == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return - (p * np.log2(p) + (1 - p) * np.log2(1-p))\n",
    "\n",
    "def information_gain(left_child_node, right_child_node):\n",
    "    \"\"\"Difference of Impurity score between parent and child nodes.\"\"\"\n",
    "    parent = left_child_node + right_child_node\n",
    "    p_parent = parent.count(1) / len(parent) if len(parent) > 0 else 0\n",
    "    p_left = left_child_node.count(1) / len(left_child_node) if len(left_child_node) > 0 else 0\n",
    "    p_right = right_child_node.count(1) / len(right_child_node) if len(right_child_node) > 0 else 0\n",
    "    IG_p = entropy(p_parent)\n",
    "    IG_l = entropy(p_left)\n",
    "    IG_r = entropy(p_right)\n",
    "    return IG_p - len(left_child_node) / len(parent) * IG_l - len(right_child_node) / len(parent) * IG_r\n",
    "\n",
    "def gini_impurity(p):\n",
    "    \"\"\"An impurity score based on probabilty of labels residing in a node.\"\"\"\n",
    "    return 1 - np.square(p)\n",
    "\n",
    "def gini_score(left_child_node, right_child_node):\n",
    "    parent = left_child_node + right_child_node\n",
    "    p_parent = parent.count(1) / len(parent) if len(parent) > 0 else 0\n",
    "    p_left = left_child_node.count(1) / len(left_child_node) if len(left_child_node) > 0 else 0\n",
    "    p_right = right_child_node.count(1) / len(right_child_node) if len(right_child_node) > 0 else 0\n",
    "    gini_l = gini_impurity(p_left)\n",
    "    gini_r = gini_impurity(p_right)\n",
    "    return gini_l + gini_r\n",
    "\n",
    "def draw_bootstrap(X_train, y_train):\n",
    "    \"\"\"Boostraping (Drawing random data samples with Replacement) the given dataset.\"\"\"\n",
    "    bootstrap_indices = list(np.random.choice(range(len(X_train)), len(X_train), replace = True))\n",
    "    oob_indices = [i for i in range(len(X_train)) if i not in bootstrap_indices]\n",
    "    X_bs = X_train.iloc[bootstrap_indices].values\n",
    "    y_bs = y_train[bootstrap_indices]\n",
    "    X_oob = X_train.iloc[oob_indices].values\n",
    "    y_oob = y_train[oob_indices]\n",
    "    return X_bs, y_bs, X_oob, y_oob\n",
    "\n",
    "def oob_score(tree, X_test, y_test):\n",
    "    \"\"\"OOB score is computed as the number of correctly predicted rows from the out of bag sample.\n",
    "    Out-Of-Bag (OOB) error is the average error for each, zi, calculated using predictions from the \n",
    "    trees that do not contain, zi, in their respective bootstrap sample.\"\"\"\n",
    "    mis_label = 0\n",
    "    for i in range(len(X_test)):\n",
    "        pred = predict_tree(tree, X_test[i])\n",
    "        if pred != y_test[i]:\n",
    "            mis_label += 1\n",
    "    return mis_label / len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_split_point(X_bootstrap, y_bootstrap, max_features, impurity):\n",
    "    \"\"\"Given maximum no. of features at a node try to find the best split point according\n",
    "    to impurity score for each threshold or possible split point.\"\"\"\n",
    "    feature_ls = list()\n",
    "    num_features = len(X_bootstrap[0])\n",
    "    \n",
    "    while len(feature_ls) <= max_features:\n",
    "        feature_idx = random.sample(range(num_features), 1)\n",
    "        if feature_idx not in feature_ls:\n",
    "            feature_ls.extend(feature_idx)\n",
    "    \n",
    "    best_info_gain = -999\n",
    "    node = None\n",
    "    for feature_idx in feature_ls:\n",
    "        for split_point in X_bootstrap[:,feature_idx]:\n",
    "            left_child_node = {'X_bootstrap': [], 'y_bootstrap': []}\n",
    "            right_child_node = {'X_bootstrap': [], 'y_bootstrap': []}\n",
    "            \n",
    "            # split children for continuous variables\n",
    "            if type(split_point) in [int, float]:\n",
    "                for i, value in enumerate(X_bootstrap[:,feature_idx]):\n",
    "                    if value <= split_point:\n",
    "                        left_child_node['X_bootstrap'].append(X_bootstrap[i])\n",
    "                        left_child_node['y_bootstrap'].append(y_bootstrap[i])\n",
    "                    else:\n",
    "                        right_child_node['X_bootstrap'].append(X_bootstrap[i])\n",
    "                        right_child_node['y_bootstrap'].append(y_bootstrap[i])\n",
    "            # split children for categoric variables\n",
    "            else:\n",
    "                for i, value in enumerate(X_bootstrap[:,feature_idx]):\n",
    "                    if value == split_point:\n",
    "                        left_child_node['X_bootstrap'].append(X_bootstrap[i])\n",
    "                        left_child_node['y_bootstrap'].append(y_bootstrap[i])\n",
    "                    else:\n",
    "                        right_child_node['X_bootstrap'].append(X_bootstrap[i])\n",
    "                        right_child_node['y_bootstrap'].append(y_bootstrap[i])\n",
    "            \n",
    "            if impurity=='entropy':\n",
    "                split_info_gain = information_gain(left_child_node['y_bootstrap'], right_child_node['y_bootstrap'])\n",
    "            else:\n",
    "                split_info_gain = gini_score(left_child_node['y_bootstrap'], right_child_node['y_bootstrap'])\n",
    "            \n",
    "            if split_info_gain > best_info_gain:\n",
    "                best_info_gain = split_info_gain\n",
    "                left_child_node['X_bootstrap'] = np.array(left_child_node['X_bootstrap'])\n",
    "                right_child_node['X_bootstrap'] = np.array(right_child_node['X_bootstrap'])\n",
    "                node = {'information_gain': split_info_gain, \n",
    "                        'left_child': left_child_node, \n",
    "                        'right_child': right_child_node, \n",
    "                        'split_point': split_point,\n",
    "                        'feature_idx': feature_idx}\n",
    "                \n",
    "    \n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terminal_node(node):\n",
    "    \"\"\"Based on max count of labels/class, assign the label to leaf node\"\"\"\n",
    "    y_bootstrap = node['y_bootstrap']\n",
    "    pred = max(y_bootstrap, key = y_bootstrap.count)\n",
    "    return pred\n",
    "\n",
    "\n",
    "def split_node(node, max_features, min_samples_split, max_depth, depth, impurity):\n",
    "    \"\"\"Based on Impurity score try to split the node \n",
    "    subject to constraints like max_features, min_samples_spit, max_depth and depth.\"\"\"\n",
    "    left_child = node['left_child']\n",
    "    right_child = node['right_child']\n",
    "\n",
    "    del(node['left_child'])\n",
    "    del(node['right_child'])\n",
    "\n",
    "    if len(left_child['y_bootstrap']) == 0 or len(right_child['y_bootstrap']) == 0:\n",
    "        empty_child = {'y_bootstrap': left_child['y_bootstrap'] + right_child['y_bootstrap']}\n",
    "        node['left_split'] = terminal_node(empty_child)\n",
    "        node['right_split'] = terminal_node(empty_child)\n",
    "        return\n",
    "\n",
    "    if depth >= max_depth:\n",
    "        node['left_split'] = terminal_node(left_child)\n",
    "        node['right_split'] = terminal_node(right_child)\n",
    "        return node\n",
    "\n",
    "    if len(left_child['X_bootstrap']) <= min_samples_split:\n",
    "        node['left_split'] = node['right_split'] = terminal_node(left_child)\n",
    "    else:\n",
    "        node['left_split'] = find_split_point(left_child['X_bootstrap'], left_child['y_bootstrap'], max_features, impurity)\n",
    "        split_node(node['left_split'], max_depth, min_samples_split, max_depth, depth + 1, impurity)\n",
    "    if len(right_child['X_bootstrap']) <= min_samples_split:\n",
    "        node['right_split'] = node['left_split'] = terminal_node(right_child)\n",
    "    else:\n",
    "        node['right_split'] = find_split_point(right_child['X_bootstrap'], right_child['y_bootstrap'], max_features, impurity)\n",
    "        split_node(node['right_split'], max_features, min_samples_split, max_depth, depth + 1, impurity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def build_tree(X_bootstrap, y_bootstrap, max_depth, min_samples_split, max_features, impurity):\n",
    "    \"\"\"Build decision tree given boostrapped data along with its labels \n",
    "    and the criteria to split + some extra factors.\"\"\"\n",
    "    root_node = find_split_point(X_bootstrap, y_bootstrap, max_features, impurity)\n",
    "    split_node(root_node, max_features, min_samples_split, max_depth, 1, impurity)\n",
    "    return root_node\n",
    "\n",
    "def random_forest(X_train, y_train, n_estimators, max_features, max_depth, min_samples_split, impurity):\n",
    "    \"\"\"Construct Random Forest using a bunch of decision trees.\"\"\"\n",
    "    tree_ls = list()\n",
    "    oob_ls = list()\n",
    "    for i in range(n_estimators):\n",
    "        X_bootstrap, y_bootstrap, X_oob, y_oob = draw_bootstrap(X_train, y_train)\n",
    "        tree = build_tree(X_bootstrap, y_bootstrap, max_features, max_depth, min_samples_split, impurity)\n",
    "        tree_ls.append(tree)\n",
    "        oob_error = oob_score(tree, X_oob, y_oob)\n",
    "        oob_ls.append(oob_error)\n",
    "    print(\"OOB estimate for Randon Forest model: {:.2f}\".format(np.mean(oob_ls)))\n",
    "    return tree_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def predict_tree(tree, x_test_sample):\n",
    "    \"\"\"Predict the output for a single sample with a given tree.\"\"\"\n",
    "    feature_idx = tree['feature_idx']\n",
    "\n",
    "    if x_test_sample[feature_idx] <= tree['split_point']:\n",
    "        if type(tree['left_split']) == dict:\n",
    "            return predict_tree(tree['left_split'], x_test_sample)\n",
    "        else:\n",
    "            value = tree['left_split']\n",
    "            return value\n",
    "    else:\n",
    "        if type(tree['right_split']) == dict:\n",
    "            return predict_tree(tree['right_split'], x_test_sample)\n",
    "        else:\n",
    "            return tree['right_split']\n",
    "        \n",
    "def predict_all_tree(tree, X_test):\n",
    "    \"\"\"Return prediction for all samples in X_test\"\"\"\n",
    "    if type(X_test) != np.ndarray:\n",
    "        X_test = X_test.values\n",
    "    return np.array([predict_tree(tree, X_test[i]) for i in range(len(X_test))])\n",
    "    \n",
    "def predict_rf(tree_ls, X_test):\n",
    "    \"\"\"Return all the predictions for test data using all trees in Random forest.\"\"\"\n",
    "    pred_ls = list()\n",
    "    if type(X_test) != np.ndarray:\n",
    "        X_test = X_test.values\n",
    "    for i in range(len(X_test)):\n",
    "        ensemble_preds = [predict_tree(tree, X_test[i]) for tree in tree_ls]\n",
    "        final_pred = max(ensemble_preds, key = ensemble_preds.count)\n",
    "        pred_ls.append(final_pred)\n",
    "    return np.array(pred_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJOn5nNZHMOU"
   },
   "source": [
    "4. Report the accuracies obtained after using the Random forest algorithm and compare it with the best accuracies obtained with the decision trees. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For `Gini`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB estimate for Randon Forest model: 0.19\n",
      "============================================================================\n",
      "With Gini (as Impurity score):\n",
      "\n",
      "Best accuracy obtained with Decision Tree: 0.931\n",
      "Accuracy obatined with Random Forets: 0.926\n",
      "============================================================================\n"
     ]
    }
   ],
   "source": [
    "model_rf_gini = random_forest(X_train, y_train, n_estimators=10, max_features=3,\\\n",
    "                      max_depth=3, min_samples_split=2, impurity='gini')\n",
    "print(\"============================================================================\")\n",
    "# Prediction for RF with Maximum voting strategy.\n",
    "preds = predict_rf(model_rf_gini, X_test)\n",
    "# Predict accuracy.\n",
    "acc_rf = accuracy_score(y_test, preds)\n",
    "acc_tree = [accuracy_score(y_test, predict_all_tree(d_tree, X_test)) for d_tree in model_rf_gini]\n",
    "print(\"With Gini (as Impurity score):\\n\\nBest accuracy obtained with Decision Tree: {0:.3f}\\nAccuracy obtained with Random Forest: {1:.3f}\".format(max(acc_tree), acc_rf))\n",
    "print(\"============================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For `Entropy`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB estimate for Randon Forest model: 0.16\n",
      "============================================================================\n",
      "With Entropy (as Impurity score):\n",
      "\n",
      "Best accuracy obtained with Decision Tree: 0.939\n",
      "Accuracy obatined with Random Forets: 0.494\n",
      "============================================================================\n"
     ]
    }
   ],
   "source": [
    "model_rf_entropy = random_forest(X_train, y_train, n_estimators=10, max_features=3,\\\n",
    "                      max_depth=3, min_samples_split=2, impurity='entropy')\n",
    "print(\"============================================================================\")\n",
    "# Prediction for RF with Maximum voting strategy.\n",
    "preds = predict_rf(model_rf, X_test)\n",
    "# Predict accuracy.\n",
    "acc_rf = accuracy_score(y_test, preds)\n",
    "acc_tree = [accuracy_score(y_test, predict_all_tree(d_tree, X_test)) for d_tree in model_rf_entropy]\n",
    "print(\"With Entropy (as Impurity score):\\n\\nBest accuracy obtained with Decision Tree: {0:.3f}\\nAccuracy obtained with Random Forest: {1:.3f}\".format(max(acc_tree), acc_rf))\n",
    "print(\"============================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yj-vNvsYHMOX"
   },
   "source": [
    "5. Submit your solution as a separate pdf in the final zip file of your submission\n",
    "\n",
    "\n",
    "Compute a decision tree with the goal to predict the food review based on its smell, taste and portion size.\n",
    "\n",
    "(a) Compute the entropy of each rule in the first stage.\n",
    "\n",
    "(b) Show the final decision tree. Clearly draw it.\n",
    "\n",
    "Submit a handwritten response. Clearly show all the steps.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "DecisionTreesAndRandomForests.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "1f8d80d535cfd832283e4e3a1095d2ce45fe6627336684f2622a1965babb2f1c"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
